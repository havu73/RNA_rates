{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This file acts like my draft papers for developing the solver. Once I figure out something, the code will be put into production in the .py files\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c3e89c0bce5835"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "SEED= 9999\n",
    "np.random.seed(SEED)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "ONE_KB = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-19T19:04:08.675844Z",
     "start_time": "2024-08-19T19:04:08.675237Z"
    }
   },
   "id": "d2d995a37a85c454"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2726585/3681833607.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  A = torch.load(A_fn)  # (n, m)\n"
     ]
    }
   ],
   "source": [
    "A_fn = '/gladstone/engelhardt/home/hvu/source/RNA_rates/splicingrates/simulations/elong_analysis/A0.33_0.2.pt'\n",
    "A = torch.load(A_fn)  # (n, m)\n",
    "b = torch.ones((A.shape[0], 1)) * 5 # (n, 1)\n",
    "coordinates = 0.2* torch.arange(A.shape[1])+0.1 # (m, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-19T19:11:32.408270Z",
     "start_time": "2024-08-19T19:11:32.367202Z"
    }
   },
   "id": "43a0ca6cdda29671"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2726585/1042685630.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(b)\n",
      "/tmp/ipykernel_2726585/1042685630.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coordinates = torch.tensor(coordinates)\n",
      "/tmp/ipykernel_2726585/1042685630.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean_prior = torch.tensor(mean_prior)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 53\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[1;32m     52\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 53\u001B[0m     output \u001B[38;5;241m=\u001B[39m model(X)\n\u001B[1;32m     54\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mmll(output, y)\n\u001B[1;32m     55\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/anaconda3/envs/new_rna/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:268\u001B[0m, in \u001B[0;36mExactGP.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    264\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m    265\u001B[0m             torch\u001B[38;5;241m.\u001B[39mequal(train_input, \u001B[38;5;28minput\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m train_input, \u001B[38;5;28minput\u001B[39m \u001B[38;5;129;01min\u001B[39;00m length_safe_zip(train_inputs, inputs)\n\u001B[1;32m    266\u001B[0m         ):\n\u001B[1;32m    267\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou must train on the training inputs!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 268\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[1;32m    271\u001B[0m \u001B[38;5;66;03m# Prior mode\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_rna/lib/python3.11/site-packages/gpytorch/module.py:31\u001B[0m, in \u001B[0;36mModule.__call__\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tensor, Distribution, LinearOperator]:\n\u001B[0;32m---> 31\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m     33\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "Cell \u001B[0;32mIn[11], line 28\u001B[0m, in \u001B[0;36mBayesianLinearRegressionGP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 28\u001B[0m     mean_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_module(x)\n\u001B[1;32m     29\u001B[0m     covar_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcovar_module(x, x)\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gpytorch\u001B[38;5;241m.\u001B[39mdistributions\u001B[38;5;241m.\u001B[39mMultivariateNormal(mean_x, covar_x)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_rna/lib/python3.11/site-packages/gpytorch/means/mean.py:22\u001B[0m, in \u001B[0;36mMean.__call__\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndimension() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     20\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 22\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(Mean, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(x)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m~/anaconda3/envs/new_rna/lib/python3.11/site-packages/gpytorch/module.py:31\u001B[0m, in \u001B[0;36mModule.__call__\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tensor, Distribution, LinearOperator]:\n\u001B[0;32m---> 31\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m     33\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [_validate_module_outputs(output) \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[0;32m~/anaconda3/envs/new_rna/lib/python3.11/site-packages/gpytorch/means/linear_mean.py:18\u001B[0m, in \u001B[0;36mLinearMean.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 18\u001B[0m     res \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mmatmul(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     20\u001B[0m         res \u001B[38;5;241m=\u001B[39m res \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import RBFKernel\n",
    "\n",
    "class CustomKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, coordinates, X):\n",
    "        super().__init__()\n",
    "        self.coordinates = coordinates\n",
    "        self.X = X  # (n, m) where n: # samples, m: # features\n",
    "        self.base_kernel = RBFKernel()\n",
    "    def forward(self):\n",
    "        # Compute RBF kernel for coordinates\n",
    "        coord_covar = self.base_kernel(self.coordinates, self.coordinates)\n",
    "        # Compute X^T * RBF(coordinates) * X\n",
    "        return self.X @ coord_covar @ self.X.t()\n",
    "\n",
    "class BayesianLinearRegressionGP(ExactGP):\n",
    "    def __init__(self, train_x, train_y, coordinates, mean_prior, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = LinearMean(input_size=train_x.size(-1))\n",
    "        self.covar_module = CustomKernel(coordinates, train_x)\n",
    "        self.mean_module.weights.data = mean_prior\n",
    "        self.mean_module.bias.data.fill_(0.0)\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x, x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "# Assuming X, Y, coordinates, and mean_prior are your data\n",
    "X = torch.tensor(A)\n",
    "y = torch.tensor(b)\n",
    "coordinates = torch.tensor(coordinates)\n",
    "mean_prior = 0.33 * torch.ones(coordinates.shape)  # Prior mean is 0\n",
    "mean_prior = torch.tensor(mean_prior)\n",
    "\n",
    "likelihood = GaussianLikelihood()\n",
    "model = BayesianLinearRegressionGP(X, y, coordinates, mean_prior, likelihood)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = -mll(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:48.992979Z",
     "start_time": "2024-08-19T19:46:48.942154Z"
    }
   },
   "id": "5b45711e91c12491"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# eventually, I want to draw a heatmap of A"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e9600b0989c34c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "new_rna",
   "language": "python",
   "display_name": "Python (new_rna)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
